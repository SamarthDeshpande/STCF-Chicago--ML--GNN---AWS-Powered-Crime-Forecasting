{
	"jobConfig": {
		"name": "chicago-preprocessing-job",
		"description": "Preprocessing crime data from raw CSV to cleaned Parquet",
		"role": "arn:aws:iam::110860223215:role/service-role/AWSGlueServiceRole-core",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "crime_preprocessing.py",
		"scriptLocation": "s3://dataset-chicago/project-folder/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-29T21:14:39.823Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://dataset-chicago/temp/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-110860223215-ca-central-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# ============================================================\r\n# ðŸ§¹ Chicago Crime â€” Stage 1 Preprocessing (PySpark, S3-ready)\r\n# - No Python UDFs (vectorized Spark only)\r\n# - No data leakage in rolling/window features\r\n# - Safe column guards; robust timestamp parsing\r\n# - Athena/QuickSight-friendly Parquet (snappy, partitioned)\r\n# ============================================================\r\n\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import (\r\n    col, lit, when, upper, trim, coalesce, to_timestamp, year, month, hour, dayofweek,\r\n    count, mean, round as sround, concat_ws, instr, regexp_replace\r\n)\r\nfrom pyspark.sql.window import Window\r\n\r\n# ========== 1) Spark ==========\r\nspark = SparkSession.builder \\\r\n    .appName(\"CrimeDataPreprocessing\") \\\r\n    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\r\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\r\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\r\n    .getOrCreate()\r\n\r\n# ---- Paths (edit if needed) ----\r\ninput_path  = \"s3a://dataset-chicago/project-folder/Dataset/dataset.csv\"    # use s3a\r\noutput_path = \"s3a://dataset-chicago/project-folder/Processed/Stage1/\"\r\n\r\n# ========== 2) Load ==========\r\ndf = spark.read.csv(input_path, header=True, inferSchema=True)\r\n\r\n# Utility: guard for optional columns\r\ndef has(cname: str) -> bool:\r\n    return cname in df.columns\r\n\r\n# ========== 3) Drop sparse/irrelevant ==========\r\nsparse_cols = [\"Ward\", \"Community Area\"]\r\ndrop_cols   = [\"ID\", \"Case Number\", \"Block\", \"IUCR\", \"X Coordinate\", \"Y Coordinate\"]\r\n\r\ndf = df.drop(*[c for c in sparse_cols if has(c)])\r\ndf = df.drop(*[c for c in drop_cols   if has(c)])\r\ndf = df.dropDuplicates()\r\n\r\n# ========== 4) Clean Description ==========\r\nif has(\"Description\"):\r\n    df = df.withColumn(\"Description\", upper(trim(col(\"Description\"))))\r\n\r\n# ========== 5) Timestamp parse + time parts ==========\r\nif not has(\"Date\"):\r\n    raise ValueError(\"Missing required column: 'Date'\")\r\n\r\ndf = df.withColumn(\r\n    \"Date_TS\",\r\n    coalesce(\r\n        to_timestamp(col(\"Date\"), \"MM/dd/yyyy hh:mm:ss a\"),\r\n        to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\")\r\n    )\r\n)\r\ndf = df.filter(col(\"Date_TS\").isNotNull())\r\n\r\ndf = df.withColumn(\"Year\",    year(\"Date_TS\")) \\\r\n       .withColumn(\"Month\",   month(\"Date_TS\")) \\\r\n       .withColumn(\"Hour\",    hour(\"Date_TS\")) \\\r\n       .withColumn(\"Weekday\", dayofweek(\"Date_TS\")) \\\r\n       .withColumn(\"IsWeekend\", when(col(\"Weekday\").isin(1,7), lit(1)).otherwise(lit(0)))\r\n\r\n# ========== 6) Season & TimeSlot ==========\r\ndf = df.withColumn(\r\n    \"Season\",\r\n    when(col(\"Month\").isin(12,1,2),  lit(\"Winter\"))\r\n    .when(col(\"Month\").isin(3,4,5),  lit(\"Spring\"))\r\n    .when(col(\"Month\").isin(6,7,8),  lit(\"Summer\"))\r\n    .otherwise(lit(\"Fall\"))\r\n)\r\n\r\ndf = df.withColumn(\r\n    \"TimeSlot\",\r\n    when((col(\"Hour\") <= 5) | (col(\"Hour\") >= 23), lit(\"Night\"))\r\n    .when(col(\"Hour\").between(6,11),               lit(\"Morning\"))\r\n    .when(col(\"Hour\").between(12,17),              lit(\"Afternoon\"))\r\n    .otherwise(lit(\"Evening\"))\r\n)\r\n\r\n# ========== 7) YearGroup & Special Eras ==========\r\ndf = df.withColumn(\r\n    \"YearGroup\",\r\n    when(col(\"Year\").between(2001,2005), lit(\"2001â€“2005\"))\r\n    .when(col(\"Year\").between(2006,2010), lit(\"2006â€“2010\"))\r\n    .when(col(\"Year\").between(2011,2015), lit(\"2011â€“2015\"))\r\n    .when(col(\"Year\").between(2016,2020), lit(\"2016â€“2020\"))\r\n    .otherwise(lit(\"2021â€“2025\"))\r\n)\r\n\r\ndf = df.withColumn(\"Is_Covid_Era\",  when(col(\"Year\").isin(2020, 2021), lit(1)).otherwise(lit(0))) \\\r\n       .withColumn(\"Is_Post_Reform\", when(col(\"Year\") >= 2016,          lit(1)).otherwise(lit(0)))\r\n\r\n# ========== 8) PrimaryType & CrimeType ==========\r\nif has(\"Primary Type\"):\r\n    df = df.withColumn(\"PrimaryType\", upper(trim(col(\"Primary Type\"))))\r\nelif has(\"PrimaryType\"):\r\n    df = df.withColumn(\"PrimaryType\", upper(trim(col(\"PrimaryType\"))))\r\nelse:\r\n    raise ValueError(\"Missing required column: 'Primary Type' (or 'PrimaryType').\")\r\n\r\ntop_crimes = ['THEFT', 'BATTERY', 'CRIMINAL DAMAGE', 'ASSAULT', 'NARCOTICS',\r\n              'BURGLARY', 'MOTOR VEHICLE THEFT', 'ROBBERY', 'DECEPTIVE PRACTICE']\r\n\r\ndf = df.withColumn(\r\n    \"CrimeType\",\r\n    when(col(\"PrimaryType\").isin(*top_crimes), col(\"PrimaryType\")).otherwise(lit(\"OTHER\"))\r\n)\r\n\r\n# ========== 9) CrimeType frequency (prior-only) ==========\r\ncrime_w = Window.partitionBy(\"CrimeType\") \\\r\n                .orderBy(col(\"Date_TS\").cast(\"long\")) \\\r\n                .rowsBetween(Window.unboundedPreceding, -1)\r\ndf = df.withColumn(\"CrimeType_Count\", count(lit(1)).over(crime_w))\r\n\r\n# ========== 10) Description_Grouped (SAFE rules only) ==========\r\nif has(\"Description\"):\r\n    desc_u = upper(trim(col(\"Description\")))\r\n    df = df.withColumn(\r\n        \"Description_Grouped\",\r\n        when(instr(desc_u, \"UNDER\") > 0,    lit(\"UNDER_500\"))\r\n        .when(instr(desc_u, \"OVER\") > 0,    lit(\"OVER_500\"))\r\n        .when(instr(desc_u, \"DOMESTIC\") > 0,lit(\"DOMESTIC\"))\r\n        .otherwise(lit(\"OTHER\"))\r\n    )\r\nelse:\r\n    df = df.withColumn(\"Description_Grouped\", lit(\"OTHER\"))\r\n\r\n# ========== 11) Location grouping ==========\r\nloc_src = \"Location Description\" if has(\"Location Description\") else (\"Location_Description\" if has(\"Location_Description\") else None)\r\nif loc_src:\r\n    df = df.withColumn(\"Location_Description\", coalesce(upper(trim(col(loc_src))), lit(\"UNKNOWN\")))\r\n    if loc_src != \"Location_Description\":\r\n        df = df.drop(loc_src)\r\nelse:\r\n    df = df.withColumn(\"Location_Description\", lit(\"UNKNOWN\"))\r\n\r\ntop_locs = [\r\n    \"STREET\", \"RESIDENCE\", \"APARTMENT\", \"SIDEWALK\", \"PARKING LOT/GARAGE(NON.RESID.)\",\r\n    \"GAS STATION\", \"SCHOOL - PUBLIC\", \"RESTAURANT\", \"CTA PLATFORM\", \"ALLEY\"\r\n]\r\ndf = df.withColumn(\r\n    \"Location_Grouped\",\r\n    when(col(\"Location_Description\").isin(*top_locs), col(\"Location_Description\")).otherwise(lit(\"OTHER\"))\r\n)\r\n\r\n# ========== 12) Binary flags & context ==========\r\ndef to_int_flag(src_col: str, out_col: str):\r\n    global df\r\n    if has(src_col):\r\n        df = df.withColumn(src_col, regexp_replace(upper(trim(col(src_col))), \"TRUE\", \"1\"))\r\n        df = df.withColumn(src_col, regexp_replace(upper(trim(col(src_col))), \"FALSE\", \"0\"))\r\n        df = df.withColumn(out_col, col(src_col).cast(\"int\"))\r\n    else:\r\n        df = df.withColumn(out_col, lit(0))\r\n\r\nto_int_flag(\"Arrest\",   \"Arrest_Flag\")\r\nto_int_flag(\"Domestic\", \"Domestic_Flag\")\r\n\r\ndf = df.withColumn(\"Crime_Context\", concat_ws(\"_\", col(\"CrimeType\"), col(\"Domestic_Flag\")))\r\n\r\n# ========== 13) Arrest rate by (Beat, CrimeType) â€” prior-only ==========\r\nif not has(\"Beat\"):\r\n    raise ValueError(\"Missing required column: 'Beat'.\")\r\n\r\narrest_w = Window.partitionBy(\"Beat\", \"CrimeType\") \\\r\n                 .orderBy(col(\"Date_TS\").cast(\"long\")) \\\r\n                 .rowsBetween(Window.unboundedPreceding, -1)\r\n\r\ndf = df.withColumn(\r\n    \"Arrest_Rate_Percent\",\r\n    sround(mean(col(\"Arrest_Flag\")).over(arrest_w) * 100.0, 2)\r\n)\r\n\r\n# ========== 14) Domestic rate by Beat â€” prior-only ==========\r\ndom_w = Window.partitionBy(\"Beat\") \\\r\n              .orderBy(col(\"Date_TS\").cast(\"long\")) \\\r\n              .rowsBetween(Window.unboundedPreceding, -1)\r\n\r\ndf = df.withColumn(\r\n    \"Domestic_Crime_Rate_Percent\",\r\n    sround(mean(col(\"Domestic_Flag\")).over(dom_w) * 100.0, 2)\r\n)\r\n\r\n# ========== 15) FBI code features (safe .like prefix checks) ==========\r\nif has(\"FBI Code\"):\r\n    df = df.withColumn(\"FBI_Code\", upper(trim(col(\"FBI Code\"))))\r\nelif has(\"FBI_Code\"):\r\n    df = df.withColumn(\"FBI_Code\", upper(trim(col(\"FBI_Code\"))))\r\nelse:\r\n    df = df.withColumn(\"FBI_Code\", lit(\"UNKNOWN\"))\r\n\r\ndf = df.withColumn(\r\n    \"FBI_Category\",\r\n    when(col(\"FBI_Code\").like(\"06%\"),   lit(\"THEFT\"))\r\n    .when(col(\"FBI_Code\").like(\"08B%\"), lit(\"ASSAULT\"))\r\n    .when(col(\"FBI_Code\").like(\"14%\"),  lit(\"VANDALISM\"))\r\n    .when(col(\"FBI_Code\").like(\"18%\"),  lit(\"DRUG VIOLATION\"))\r\n    .when(col(\"FBI_Code\").like(\"26%\"),  lit(\"WEAPONS\"))\r\n    .when(col(\"FBI_Code\").like(\"01A%\"), lit(\"HOMICIDE\"))\r\n    .otherwise(lit(\"OTHER\"))\r\n)\r\n\r\nfbi_w = Window.partitionBy(\"FBI_Code\") \\\r\n              .orderBy(col(\"Date_TS\").cast(\"long\")) \\\r\n              .rowsBetween(Window.unboundedPreceding, -1)\r\n\r\ndf = df.withColumn(\"FBI_Code_Count\", count(lit(1)).over(fbi_w)) \\\r\n       .withColumn(\"FBI_Arrest_Rate\", sround(mean(col(\"Arrest_Flag\")).over(fbi_w) * 100.0, 2))\r\n\r\n# ========== 16) Geo bins ==========\r\nif has(\"Latitude\") and has(\"Longitude\"):\r\n    df = df.filter(col(\"Latitude\").isNotNull() & col(\"Longitude\").isNotNull())\r\n    df = df.withColumn(\"Lat_Bin\", sround(col(\"Latitude\"), 2)) \\\r\n           .withColumn(\"Lng_Bin\", sround(col(\"Longitude\"), 2))\r\nelse:\r\n    df = df.withColumn(\"Lat_Bin\", lit(None).cast(\"double\")) \\\r\n           .withColumn(\"Lng_Bin\", lit(None).cast(\"double\"))\r\n\r\n# ========== 17) Null handling for counts ==========\r\ncount_cols = [c for c in [\"CrimeType_Count\", \"FBI_Code_Count\"] if has(c)]\r\nif count_cols:\r\n    df = df.fillna(0, subset=count_cols)\r\n\r\n# Defensive dedup\r\ndf = df.dropDuplicates()\r\n\r\n# ========== 18) Save to Parquet (partitioned) ==========\r\ndf.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\").parquet(output_path)\r\n\r\nprint(\"âœ… Preprocessing complete. Final row count:\", df.count())\r\nspark.stop()\r\n"
}