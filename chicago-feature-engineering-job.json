{
	"jobConfig": {
		"name": "chicago-feature-engineering-job",
		"description": "",
		"role": "arn:aws:iam::110860223215:role/service-role/AWSGlueServiceRole-core",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "crime_feature_engineering.py",
		"scriptLocation": "s3://dataset-chicago/project-folder/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-29T21:26:13.582Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://dataset-chicago/temp/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-110860223215-ca-central-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# ============================\r\n# Chicago Crime â€” Feature Engineering (Stage2, aligned to Stage1)\r\n# - Uses Stage1 columns exactly (Beat, Date_TS, CrimeType, Hour, Arrest_Flag, Year, Month, Lat_Bin, Lng_Bin)\r\n# - No data leakage: all windows use rowsBetween(-inf, -1) or lag()\r\n# - Athena/QuickSight-friendly Parquet (snappy, partitioned by Year, Month)\r\n# ============================\r\n\r\nfrom pyspark.sql import SparkSession\r\nimport pyspark.sql.functions as F\r\nfrom pyspark.sql.window import Window\r\n\r\n# ------------------------------------------\r\n# ðŸš€ Initialize Spark Session (Glue 4.0 safe)\r\n# ------------------------------------------\r\nspark = (\r\n    SparkSession.builder\r\n    .appName(\"CrimeFeatureEngineering_Stage2\")\r\n    .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\r\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\r\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n    .getOrCreate()\r\n)\r\n\r\n# ------------------------------------------\r\n# ðŸ“‚ Input / Output Paths\r\n# ------------------------------------------\r\ninput_path  = \"s3a://dataset-chicago/project-folder/Processed/Stage1/\"\r\noutput_path = \"s3a://dataset-chicago/project-folder/Processed/Final/\"\r\n\r\n# ------------------------------------------\r\n# ðŸ§¾ Load Stage1 Preprocessed Data\r\n# ------------------------------------------\r\ndf = spark.read.parquet(input_path).dropDuplicates()\r\n\r\n# Ensure Stage1 columns exist (created during preprocessing)\r\n# Stage1 guarantees these fields: Beat, Date_TS, CrimeType, Hour, Arrest_Flag, Year, Month, Lat_Bin, Lng_Bin,\r\n# plus many others like IsWeekend, Domestic_Flag, FBI_Code, etc.\r\nrequired_cols = [\"Beat\", \"Date_TS\", \"CrimeType\", \"Hour\", \"Arrest_Flag\", \"Year\", \"Month\", \"Lat_Bin\", \"Lng_Bin\"]\r\nmissing = [c for c in required_cols if c not in df.columns]\r\nif missing:\r\n    raise ValueError(f\"Missing required Stage1 columns: {missing}\")\r\n\r\n# Robust timestamp & calendar helpers\r\ndf = (\r\n    df.withColumn(\"Date_TS\", F.to_timestamp(\"Date_TS\"))\r\n      .withColumn(\"Date_Only\", F.to_date(\"Date_TS\"))\r\n      .withColumn(\"Timestamp_Unix\", F.col(\"Date_TS\").cast(\"long\"))\r\n      # Use first-of-month date for safe ordering/lag (avoids lexicographic errors)\r\n      .withColumn(\"YearMonthDate\", F.trunc(\"Date_TS\", \"MONTH\"))\r\n)\r\n\r\n# ------------------------------------------\r\n# ðŸ§® 7â€‘Day Rolling Crime Count (Beat, time-aware, past-only)\r\n# ------------------------------------------\r\n# Rolling window over last 7*86400 seconds, excluding current event\r\nrolling_window = (\r\n    Window.partitionBy(\"Beat\")\r\n    .orderBy(\"Timestamp_Unix\")\r\n    .rangeBetween(-7 * 86400, -1)\r\n)\r\ndf = df.withColumn(\"Crime_Count_Last7Days\", F.count(F.lit(1)).over(rolling_window))\r\n\r\n# ------------------------------------------\r\n# ðŸ“Š CrimeTypeâ€“Hour Density P(Hour | CrimeType), past-only\r\n# ------------------------------------------\r\n# Count of prior records for (CrimeType, Hour) divided by prior total for CrimeType\r\nw_ct_total = (\r\n    Window.partitionBy(\"CrimeType\")\r\n    .orderBy(\"Timestamp_Unix\")\r\n    .rowsBetween(Window.unboundedPreceding, -1)\r\n)\r\nw_ct_hour = (\r\n    Window.partitionBy(\"CrimeType\", \"Hour\")\r\n    .orderBy(\"Timestamp_Unix\")\r\n    .rowsBetween(Window.unboundedPreceding, -1)\r\n)\r\ndf = (\r\n    df.withColumn(\"CrimeType_Total_Prior\", F.count(F.lit(1)).over(w_ct_total))\r\n      .withColumn(\"CrimeType_Hour_Prior\",  F.count(F.lit(1)).over(w_ct_hour))\r\n      .withColumn(\r\n          \"CrimeType_Hour_Density\",\r\n          F.when(F.col(\"CrimeType_Total_Prior\") > 0,\r\n                 F.col(\"CrimeType_Hour_Prior\") / F.col(\"CrimeType_Total_Prior\"))\r\n           .otherwise(F.lit(None).cast(\"double\"))\r\n      )\r\n      .drop(\"CrimeType_Total_Prior\", \"CrimeType_Hour_Prior\")\r\n)\r\n\r\n# ------------------------------------------\r\n# ðŸ“… Monthly Arrest Rate (previous month only, time-aware)\r\n# ------------------------------------------\r\n# Stage1 already created Arrest_Flag; compute per (Beat, YearMonthDate) and lag by 1 month\r\nmonthly = (\r\n    df.groupBy(\"Beat\", \"YearMonthDate\")\r\n      .agg(\r\n          F.count(F.lit(1)).alias(\"Monthly_Crime_Total\"),\r\n          F.sum(\"Arrest_Flag\").alias(\"Monthly_Arrests\")\r\n      )\r\n      .withColumn(\"Monthly_Arrest_Rate\", F.col(\"Monthly_Arrests\") / F.col(\"Monthly_Crime_Total\"))\r\n)\r\nw_month = Window.partitionBy(\"Beat\").orderBy(\"YearMonthDate\")\r\nmonthly = monthly.withColumn(\"Prev_Month_Arrest_Rate\", F.lag(\"Monthly_Arrest_Rate\", 1).over(w_month))\r\n\r\ndf = df.join(\r\n    monthly.select(\"Beat\", \"YearMonthDate\", \"Prev_Month_Arrest_Rate\"),\r\n    on=[\"Beat\", \"YearMonthDate\"],\r\n    how=\"left\"\r\n)\r\n\r\n# ------------------------------------------\r\n# ðŸ¥‡ Top CrimeType per Beat â€” previous month (time-aware & leak-safe)\r\n# ------------------------------------------\r\n# Compute top CrimeType for each (Beat, YearMonthDate), then carry \"previous month's top\" to current events.\r\nmonth_ct = (\r\n    df.groupBy(\"Beat\", \"YearMonthDate\", \"CrimeType\")\r\n      .agg(F.count(F.lit(1)).alias(\"ct_count\"))\r\n)\r\nrank_win = Window.partitionBy(\"Beat\", \"YearMonthDate\").orderBy(F.desc(\"ct_count\"), F.asc(\"CrimeType\"))\r\ntop_ct = (\r\n    month_ct.withColumn(\"r\", F.row_number().over(rank_win))\r\n            .filter(F.col(\"r\") == 1)\r\n            .select(\"Beat\", \"YearMonthDate\", F.col(\"CrimeType\").alias(\"TopCrimeType_Month\"))\r\n)\r\n# Prev-month top for leak safety\r\nw_prev_top = Window.partitionBy(\"Beat\").orderBy(\"YearMonthDate\")\r\ntop_ct = top_ct.withColumn(\"TopCrimeType_PrevMonth\", F.lag(\"TopCrimeType_Month\", 1).over(w_prev_top))\r\n\r\ndf = df.join(\r\n    top_ct.select(\"Beat\", \"YearMonthDate\", \"TopCrimeType_PrevMonth\"),\r\n    on=[\"Beat\", \"YearMonthDate\"],\r\n    how=\"left\"\r\n)\r\n\r\n# ------------------------------------------\r\n# ðŸš¨ Daily Crime Count (Beat, Lat/Lng bins) + PrevDay & Outlier flag (3Ïƒ, past-only)\r\n# ------------------------------------------\r\ndaily = (\r\n    df.groupBy(\"Beat\", \"Lat_Bin\", \"Lng_Bin\", \"Date_Only\")\r\n      .agg(F.count(F.lit(1)).alias(\"Daily_Crime_Count\"))\r\n)\r\n\r\n# Prev day count per Beat (optionally include spatial bins in partition if you want per-cell prev day)\r\nw_prev_day = (\r\n    Window.partitionBy(\"Beat\")\r\n    .orderBy(\"Date_Only\")\r\n    .rowsBetween(-1, -1)\r\n)\r\ndaily = daily.withColumn(\"PrevDay_CrimeCount\", F.first(\"Daily_Crime_Count\").over(w_prev_day))\r\n\r\n# Historical stats (mean/std) up to previous day for outlier detection\r\nw_hist = (\r\n    Window.partitionBy(\"Beat\")\r\n    .orderBy(\"Date_Only\")\r\n    .rowsBetween(Window.unboundedPreceding, -1)\r\n)\r\ndaily = (\r\n    daily.withColumn(\"hist_avg\",    F.avg(\"Daily_Crime_Count\").over(w_hist))\r\n         .withColumn(\"hist_stddev\", F.stddev(\"Daily_Crime_Count\").over(w_hist))\r\n         .withColumn(\r\n             \"Is_CrimeOutlier\",\r\n             F.when(\r\n                 (F.col(\"hist_avg\").isNotNull()) & (F.col(\"hist_stddev\").isNotNull()) &\r\n                 (F.col(\"Daily_Crime_Count\") > F.col(\"hist_avg\") + 3 * F.col(\"hist_stddev\")),\r\n                 F.lit(True)\r\n             ).otherwise(F.lit(False))\r\n         )\r\n)\r\n\r\ndf = df.join(\r\n    daily.select(\"Beat\", \"Date_Only\", \"Lat_Bin\", \"Lng_Bin\", \"PrevDay_CrimeCount\", \"Is_CrimeOutlier\"),\r\n    on=[\"Beat\", \"Date_Only\", \"Lat_Bin\", \"Lng_Bin\"],\r\n    how=\"left\"\r\n)\r\n\r\n# ------------------------------------------\r\n# âœ… Final Cleanup & Save\r\n# ------------------------------------------\r\n# Fill count-like nulls conservatively; keep rates/densities as null if unseen historically\r\ndf = df.fillna(0, subset=[\"Crime_Count_Last7Days\", \"PrevDay_CrimeCount\"])\r\n\r\n# Write Athena-optimized partitions (Stage1 used same partitioning)\r\n(\r\n    df.write.mode(\"overwrite\")\r\n      .partitionBy(\"Year\", \"Month\")\r\n      .parquet(output_path)\r\n)\r\n\r\nprint(\"âœ… Saved feature-engineered dataset to:\", output_path)\r\nprint(\"ðŸ”¢ Final row count:\", df.count())\r\n\r\nspark.stop()\r\n"
}